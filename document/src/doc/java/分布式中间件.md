
## cap理论
Consistency一致性：分布式系统中的各节点时时刻刻保持数据的一致性。
Availability可用性：可以正常的做读写操作，不会出现系统操作失败或者访问超时等问题。
Partition tolerance分区容错性：分布式系统中的某个节点或者网络分区出现了故障，整个系统仍然能对外提供满足一致性和可用性的服务。
不能同时满足CAP的原因：网络故障恢复的这段时间内，节点之间数据必然不同步。数据出现不一致，不能满足一致性；不能提供正确有效的数据，不能满足可用性。
满足CP：选择数据一致性，系统出现不可用（zookeeper、redis、hbase）。数据存储场景
满足AP：选择可用性，系统数据不一致（Nacos、12306）。提供在线服务场景
满足CA：系统不能分布式扩展（Oracle、mysql）。传统数据库场景

## BASE理论
Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）。核心思想是即使无法做到强一致性，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。  
基本可用：系统在出现不可预知故障的时候，允许损失部分可用性。响应时间增加或部分功能不可用。  
软状态：指允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。  
最终一致性：系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。  

## 选举算法
Raft
Raft将系统中的角色分为领导者（Leader，0~1个）、跟从者（Follower，1~n个）和候选人（Candidate，0~n个）；
选举：
1. 当服务器启动时，初始化为Follower。如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。
2. Follower将其当前term加一然后转换为Candidate，它首先给自己投票并且给集群中的其他服务器发送投票请求。节点在同一次投票周期内，只给一个节点投票。多数的选票，成功选举为Leader；收到了Leader的消息，表示有其它服务器已经抢先当选了Leader；没有服务器赢得多数的选票，Leader选举失败，等待选举时间超时后发起下一次选举。
3. Follower如果一段时间内没有收到Leader心跳，则认为Leader挂了，重新发起新一轮选主投票
数据同步：
1. 日志包含有序编号、当前任期、命令和前一条日志的编号和任期。如果follower找不到前一条日志的编号和任期，则拒绝该日志。
2. Leader要求Followe遵从他的指令，都将这个新的日志内容追加到他们各自日志中，大多数follower收到日志并返回确认后，Leader发出commit命令。
3. Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。
Zab
基本原理与raft一致
日志同步差异
ZooKeeper在每次leader选举完成之后，都会进行数据之间的同步纠正，所以每一个轮次，大家都日志内容都是统一的。
Raft在leader选举完成之后没有这个同步过程，而是靠之后的AppendEntries RPC请求的一致性检查来实现纠正过程，则就会出现上述案例中隔了几个轮次还不统一的现象
投票过程差异
Raft：每个轮次内server只能投票一次，哪个candidate先请求就获得投票，可能出现多个candidate获得票数一样的情况，Raft通过candidate设置随机不同的超时时间，那么先超时的先发起投票获得选举。
ZooKeeper：在每个轮次内，可以投多次票。遇到更大更新的日志则更新投票结果，通知所有人，不存在获取票数一样多的情况，但是时间会更长


## zookeeper
zookeeper：高可用、高性能、强一致性的分布式开源协调服务。适合同步服务、配置维护和集群命名和管理。
1. 存储少量数据。数据是全部存储在内存，目录树结构，节点可以存储少量数据；
2. 强一致性（CP）。成功写入多数节点；多数节点宕机或集群选举时服务不可用。
3. 写入性能差。每次需要n/2+1的节点写入，写入性能差。节点数据只能原子操作；
节点类型：1.永久节点，显式增加和删除；2.临时节点，生命周期跟session绑定，断开后会自动删除节点；
节点自增：节点支持有序自增。
核心是原子广播协议zab，保障server之间同步。
监听节点watcher：客户端可以watch节点的增、删、改操作，客户端只收到一条消息。
### 应用场景
分布式锁：1.客户端申请锁目录下创建临时有序节点，并返回该目录所有子节点；2.如果自己是排第一，则获取到锁，执行业务代码；3.如果不是排第一，则加锁失败；4.监听前一个临时节点的删除事件，直到收到通知消息；5.业务代码执行完，主动删除当前节点。如果创建改临时节点的客户端断开连接，zookeeper会清除该临时节点；6。zookeeper发现临时节点被删除，会通知监听该节点的客户端重新获取锁节点列表
命名服务：依赖顺序节点生成全局唯一命名。
全局统一配置中心：依赖watch实时监听配置变更。
注册中心：服务提供者创建临时节点，消费者watch改节点，实时监听服务变更。
服务协调：在大数据领域，Hadoop、hbase都依赖zookeeper协调节点状态和配置。


https://blog.csdn.net/suifeng3051/article/details/48053965
## kafka
基于push-subscribe的分布式消息系统。应用在消息系统、日志收集、数据采集分发、流式处理等场景。
高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒
可扩展性：kafka集群支持热扩展
持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失
容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）
高并发：支持数千个客户端同时读写

Replications数据持久化副本数量，至少为2。只有一个partition的副本会被选举成leader作为读写用。
Partition：Kafka中的topic是以partition的形式存放的，数据实际存储在log文件。Partition的数量决定了topic log的数量
一个partition只能被一个消费者消费，一个消费者可以消费多个partition。partitions >  consumers

Producers客户端设置key自己控制着消息被推送到哪些partition。支持批量发送、异步发送、结果future、acks确认副本数、
Consumers通过offset来决定读取哪条数据，通过提交offset表示消费完成的位置。
partition：每个主题可以分为多个partition，每个partition实际存储为append log；每条消息根据时间顺序分配一个单调递增的offset，以日志的形式顺序追加到文件尾部；由生产者决定消息发送到哪个partition。

### 高可用
可靠性：通过acks参数确保生产者写入成功，消费者根据offset读取和提交消息位置，保证消费成功。
备份：提高了Kafka集群的可靠性、稳定性、容错性。备份数量为n的集群允许n-1个节点失败。
### 高性能
压缩：消息头部添加了一个描述压缩属性字节，GZIP或Snappy格式对消息集合进行压缩。Producer端进行压缩之后，在Consumer端需进行解压。
持久化：依赖操作系统缓存；使用字节而不是对象，减少内存占用。  
性能：采用日志结构支持有限的操作，保证常数时间的读写性能。 
Memory Mapped Files内存文件映射：直接利用操作系统的Page来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上。 
序列化：server端直接出列字节，无需序列化和反序列化处理。  
批量：Producer批量发送消息集合；server端是以消息块的形式追加消息到log中；consumer在查询的时候也是一次查询大量的线性数据块。  
零拷贝：依赖操作系统提供的零拷贝sendfile机制，实现操作系统页面缓存和socket之间的数据传递。  

https://blog.csdn.net/dingshuo168/article/details/102970988
https://www.wwwbuild.net/huangtalkit/23928.html
## RocketMQ
commitLog日志文件：保存消息完整内容，broker上所有topic公用一个commitLog文件。
ConsumeQueue：每个topic一个，保存消息实际保存在CommitLog.offset位置；消息的tag。
indexFile：保存消息的ID，通过ID查找消息时。
offsettable.offset：ConsumeQueue的消费进度，保存在json配置文件。
### 对比RocketMQ### 顺序：
producer：客户端在发送的时候使用单线程，根据queue选择器和排序id，把有顺序关系的消息都发送到同一个queue上。
broker：客户端采用同步发送消息，broker收到消息后，顺序写入commitLog文件，保证消息在文件中是有序。
consumer：1.同一个queue只允许一个consumer消费；2.消息到达consumer后回被放进缓存队列，consumer增加了互斥锁，同一时间同一个queue只会有一个线程在处理；
3.consumer线程处理超时或失败时，该consumer会一直重试处理，直到超过最大次数，然后返还broker，broker直接把消息放死信队列，而不是重试队列。
问题：1.消息分配不均，导致消息堆积；2.broker宕机需要切换queue时，如果原consumer有消息对接，可能后发送的消息被先消费了。  
### 事务：RocketMQ支持事务，kafka不支持； 
 producer发送带有唯一key的事务消息，Broker保存到预提交队列，consumer不可见。返回事务结果给producer；
 producer执行本地事务，根据情况想broker发送commit或rollback；
 broker收到commit指令，则根据消息key从预提交队列移除消息，重新正式写入commitLog。收到rollback指令，则从预提交队列移除消息。
 broker重试未收到producer指令，会定时重试调用Producer.checkLocalTransaction，根据producer的事务状态决定commit或rollback。

### 重试：RocketMQ有重试队列和死信队列，支持失败重试机制；kafka不支持
延时消息：1.producer修改topic名称以及队列信息（根据延时级别决定发送到哪个级别的延时队列（18个level））；2.根据队列信息，转发到对应的延时Queue；3.延时服务后台线程消费延时Queue里的消息，改回真实的TOPIC；4.重新提交消息，写入commitLog；5. 把消息投递到对应的consumer Queue；  

消息查询：RocketMQ支持根据key查询消息内容，kafka不支持
消息回溯：RocketMQ支持按时间回溯；kafka支持offset回溯；

### 选型
1. 功能维度
延迟消息；顺序消息；事务消息；重试队列；死信队列；消费模式（集群、广播）；消息回溯；幂等性（At most once、At least once）
2.性能维度
吞吐量；时延
3.可靠性
消息丢失（ack）；故障恢复；集群规模；社区活跃
4.运维管理
安全、权限、监控、告警、容灾、多机房
5.生态

RocketMQ：功能丰富（分区数量、事务消息、失败重试、延迟消息、回溯、不限制消费者数量），毫秒级响应；适合在线业务场景
kafka：大数据日志场景功能丰富、兼容性好、性能高；适合日志、流式处理、大数据场景。

数据可靠性：kafka支持主从自动切换，RocketMQ不支持Master宕机Slave自动切换；  
性能：kafka写入百万级，RocketMQ十万级；kafka producer批量提交消息  
分区数量：kafka支持不超过64，影响性能，RocketMQ不影响性能；kafka每个topic partition对应一个文件，RocketMQ所有消息写入一个commitLog文件；

## Sharding-JDBC
分布式数据分片关系型数据库中间件。使用客户端jar包直连数据库，在JDBC层提供的额外服务。完全兼容JDBC规范。

## 分布式事务


#### 服务发现与治理
zookeeper（1.provider启动时在接口providers目录下创建临时子节点，写入自己的IP和端口；2.consumer启动时，订阅接口providers目录下所有子节点，并根据子节点信息解析提供者；consumer在接口consumers目录下创建临时子节点，写入自己的URL地址；3.增加提供者时，providers下面创建新临时子节点，zookeeper会推送节点信息给consumer；4.provider出现故障或下线时，由于临时节点与session有关，改临时节点会被自动删除；5.zookeeper宕机之后，服务提供者信息保存在内存和本地文件，无法感知到提供者变更
#### 分布式ID生成器
雪花算法64bit（41位，用来记录时间戳（毫秒），10位用来记录工作机器id（5位datacenterId 和 5位workerId），12位，序列号，用来记录同毫秒内产生的不同id）。
时钟回拨问题：1.抛异常；2.等待；3.设计时钟回拨位，发生回拨时+1。美团leaf（zookeeper生成workid，并缓存在本地；定时上报机器时间到zk，重启时保证当前机器时间>zk保存时间；RPC访问其他机器时间，偏差大则启动失败。1.关闭NTP机器时间同步。2.出现时钟重播时，返回异常、等待重试或下线该节点）。
#### 分布式锁
zookeeper锁：可靠性很高，强一致性。依赖zookeeper，可用性不足，效率一般。
redis锁：不可重入、单节点。加锁（SET anyLock current_client NX PX 30000），释放锁（lua脚本实现，如果value值为当前线程，则删除key）；重入锁通过hash结构来实现。（效率高，超时问题、分布式环境下数据同步问题、主从切换问题）
redlock：在2N+1台机器上，同时执行上面逻辑，N+1台机器执行成功则成功。（效率稍低，解决分布式环境下数据同步问题，维护成本高）
#### 分布式限流
guava ratelimiter：next（上一次取数的终止时间点）speed（频率每秒） maxPermits(最大申请量)。1.(current-next)*speed表示当前桶令牌数；2.如果桶令牌数小于permit，可以预支，next为预支后的时间点；3.next>current表示已经预支，当前不可获取。
分布式限流：基于redis实现，把ratelimiter的逻辑通过redis lua脚本实现。


# 微服务系统设计
